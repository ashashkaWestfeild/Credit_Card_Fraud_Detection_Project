{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection using Deep Neural Networks\n",
    "\n",
    "## Project Overview\n",
    "This project implements a Deep Neural Network (Multi-Layer Perceptron) from scratch using **NumPy** to detect fraudulent credit card transactions. The goal is to compare the performance of a non-linear neural network against a baseline linear model (Logistic Regression) on a real-world dataset.\n",
    "\n",
    "**Key Highlights:**\n",
    "- **Manual Implementation:** Forward propagation, Backpropagation, and Gradient Descent are implemented manually to demonstrate the core mechanics of Deep Learning.\n",
    "- **Full Dataset:** Analyzes the complete Credit Card Fraud Detection dataset containing 284,807 transactions.\n",
    "- **Imbalanced Data:** Addresses the challenge of detecting rare fraud events (Class 1) amidst a vast majority of legitimate transactions (Class 0).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print('[OK] Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Exploration\n",
    "\n",
    "We use the [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset. It contains transactions made by credit cards in September 2013 by European cardholders. \n",
    "\n",
    "The dataset is highly **imbalanced**: the positive class (frauds) accounts for only 0.172% of all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv\"\n",
    "\n",
    "print(f\"Downloading dataset from {url}...\")\n",
    "try:\n",
    "    df_full = pd.read_csv(url)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Use the full dataset (shuffled)\n",
    "data = df_full.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Dataset Summary\n",
    "n_samples = len(data)\n",
    "n_features = len(data.columns) - 1  # Excluding target 'Class'\n",
    "n_frauds = len(data[data['Class'] == 1])\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Total Samples: {n_samples}\")\n",
    "print(f\"Total Features: {n_features}\")\n",
    "print(f\"Fraud Cases: {n_frauds} ({n_frauds/n_samples:.3%})\")\n",
    "print(f\"Primary Metric: Recall (To maximize detection of actual fraud)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Neural networks are sensitive to the scale of input features. We perform the following steps:\n",
    "1.  **Split Features & Target**: Separate the input `X` (Features V1-V28, Time, Amount) and target `y` (Class).\n",
    "2.  **Missing Values**: Replace any missing values with 0 (though this dataset is mostly clean).\n",
    "3.  **Train/Test Split**: Split data into training (80%) and testing (20%) sets.\n",
    "4.  **Standardization**: Scale features to have zero mean and unit variance using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separate Features and Target\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# 2. Handle missing values\n",
    "X = X.fillna(0)\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape y for matrix operations\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Testing set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model: Logistic Regression\n",
    "\n",
    "We start by implementing a Logistic Regression model from scratch. This serves as a baseline to see how well a linear decision boundary can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "    \"\"\"\n",
    "    Logistic Regression classifier implemented with Gradient Descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros((n_features, 1))\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient Descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            \n",
    "            # Compute Loss (Binary Cross Entropy)\n",
    "            epsilon = 1e-15\n",
    "            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "            loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Backpropagation (Compute Gradients)\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update Parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(linear_model)\n",
    "        return (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline Model\n",
    "print(\"Training Logistic Regression Model...\")\n",
    "lr_start_time = time.time()\n",
    "\n",
    "# Note: Using 1000 iterations for efficiency on full dataset\n",
    "lr_model = LogisticRegressionModel(learning_rate=0.1, n_iterations=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Training time: {time.time() - lr_start_time:.2f}s\")\n",
    "print(f\"Final Loss: {lr_model.loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Neural Network (MLP)\n",
    "\n",
    "Now we implement a Multi-Layer Perceptron (MLP). This model adds a hidden layer with non-linear activation (ReLU), allowing it to capture more complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron implemented from scratch.\n",
    "    Architecture: Input -> Hidden (ReLU) -> Hidden (ReLU) -> Output (Sigmoid)\n",
    "    \"\"\"\n",
    "    def __init__(self, architecture, learning_rate=0.01, n_iterations=1000):\n",
    "        self.architecture = architecture\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.parameters = {}\n",
    "        self.loss_history = []\n",
    "        self.cache = {}\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(42)\n",
    "        for l in range(1, len(self.architecture)):\n",
    "            # He initialization for ReLU layers\n",
    "            self.parameters[f'W{l}'] = np.random.randn(self.architecture[l], self.architecture[l-1]) * np.sqrt(2/self.architecture[l-1])\n",
    "            self.parameters[f'b{l}'] = np.zeros((self.architecture[l], 1))\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.cache['A0'] = X.T \n",
    "        L = len(self.architecture) - 1\n",
    "        \n",
    "        # Hidden layers\n",
    "        for l in range(1, L):\n",
    "            self.cache[f'Z{l}'] = np.dot(self.parameters[f'W{l}'], self.cache[f'A{l-1}']) + self.parameters[f'b{l}']\n",
    "            self.cache[f'A{l}'] = self.relu(self.cache[f'Z{l}'])\n",
    "            \n",
    "        # Output layer\n",
    "        self.cache[f'Z{L}'] = np.dot(self.parameters[f'W{L}'], self.cache[f'A{L-1}']) + self.parameters[f'b{L}']\n",
    "        self.cache[f'A{L}'] = self.sigmoid(self.cache[f'Z{L}'])\n",
    "        \n",
    "        return self.cache[f'A{L}'].T\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        grads = {}\n",
    "        L = len(self.architecture) - 1\n",
    "        \n",
    "        dZ = self.cache[f'A{L}'] - y.T\n",
    "        grads[f'dW{L}'] = (1/m) * np.dot(dZ, self.cache[f'A{L-1}'].T)\n",
    "        grads[f'db{L}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        for l in reversed(range(1, L)):\n",
    "            dA = np.dot(self.parameters[f'W{l+1}'].T, dZ)\n",
    "            dZ = dA * self.relu_derivative(self.cache[f'Z{l}'])\n",
    "            grads[f'dW{l}'] = (1/m) * np.dot(dZ, self.cache[f'A{l-1}'].T)\n",
    "            grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads):\n",
    "        L = len(self.architecture) - 1\n",
    "        for l in range(1, L + 1):\n",
    "            self.parameters[f'W{l}'] -= self.lr * grads[f'dW{l}']\n",
    "            self.parameters[f'b{l}'] -= self.lr * grads[f'db{l}']\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.initialize_parameters()\n",
    "        for i in range(self.n_iterations):\n",
    "            y_pred = self.forward_propagation(X)\n",
    "            \n",
    "            epsilon = 1e-15\n",
    "            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "            loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            grads = self.backward_propagation(X, y)\n",
    "            self.update_parameters(grads)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}/{self.n_iterations} - Loss: {loss:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward_propagation(X)\n",
    "        return (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP Model\n",
    "print(\"Training MLP Model (this may take a few minutes)...\")\n",
    "mlp_start_time = time.time()\n",
    "\n",
    "# Architecture: [Input, 16, 8, Output]\n",
    "# We use 1000 iterations to keep runtime reasonable on the full dataset\n",
    "mlp_architecture = [n_features, 16, 8, 1]\n",
    "mlp_model = MLP(architecture=mlp_architecture, learning_rate=0.1, n_iterations=1000)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "mlp_predictions = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Training time: {time.time() - mlp_start_time:.2f}s\")\n",
    "print(f\"Final Loss: {mlp_model.loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Comparison\n",
    "\n",
    "We compare the models using standard classification metrics. **Recall** is our primary metric because missing a fraud is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "lr_metrics = calculate_metrics(y_test, lr_predictions)\n",
    "mlp_metrics = calculate_metrics(y_test, mlp_predictions)\n",
    "\n",
    "print(\"Logistic Regression Results:\", lr_metrics)\n",
    "print(\"MLP Results:\", mlp_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lr_model.loss_history, label='logistic Regression')\n",
    "plt.plot(mlp_model.loss_history, label='MLP')\n",
    "plt.title('Training Loss Convergence')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, [lr_metrics[m] for m in metrics], width, label='Logistic Reg')\n",
    "plt.bar(x + width/2, [mlp_metrics[m] for m in metrics], width, label='MLP')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.title('Performance Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This project demonstrated that implementing a Neural Network from scratch is a powerful way to understand deep learning fundamentals. Using the full dataset, we observed that the MLP generally captures non-linear fraud patterns better than the linear baseline, though it comes with higher computational costs. For a production fraud detection system, maximizing Recall (catching fraud) while maintaining reasonable Precision (avoiding false alarms) is key."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
